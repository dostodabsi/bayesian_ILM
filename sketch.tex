\documentclass[12pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[american]{babel}
\usepackage{tikz}
\usepackage{csquotes}
\usepackage{amsmath}

\usepackage{color}
\usepackage{attrib}
\usepackage{blindtext}
\usepackage[hidelinks]{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black, % references (e.g. pages)
	linkcolor=black, %table of content links etc.
	urlcolor=blue
}

\usepackage[backend=biber, style=apa]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\let\oldequation=\equation
\let\endoldequation=\endequation
\renewenvironment{equation}{\vspace{5mm}\begin{oldequation}}{\end{oldequation}\vspace{5mm}}

\setlength\bibitemsep{0.5ex}
\setlength\bibnamesep{1.2ex}
\addbibresource{bibliography.bib}

\begin{document}
\section*{Project Sketch - Dablander \& GÃ¶rner}

\subsection*{Introduction}
``One key challenge for cognitive science'', \textcite[p. 5241]{kirby2007innateness} note, ``is to explain
the structure of human language.'' One way of sheding light on this question is to consider
how children acquire language. Traditional, bio-linguistic accounts (Pinker \& Bloom, 1990),
argue that most of our language knowledge is innate, formed by natural selection. In its extreme form, most of language learning is considered illusory (Smith, 2011). Recently, these viewpoints have been revised, and focus has been put on cultural factors. Since
languages are culturally transmitted, they are potentially under selection for learnability.
The basic argument is that languages adopt to become learnable by the agents, which leads to
universal features like regularization and compositionality. The knowledge of language of one
generation gets transmitted to the next generation and so on. The innate bias of the Agent - its
cognitive constraints - form the language that we end up with.

\subsection*{Iterated Learning Model}

This process was formalized by Kirby in 1999 and termed "Iterated Learning Model". The
three core features of this model \parencite{ferdinand2008language} are

\begin{enumerate}
  \item Learning Algorithm
  \item some form of information which is the input / output of the algorithm
  \item structured transmission of information
\end{enumerate}

Put crudely, this model formalizes games like ``stille Post''. One agent starts with
a word, say elk, and passes this word on to the next agent. Each agent represents
a generation, and the word passed on represents a certain language. Not all information of
the specific language (here the word "elk") gets passed onto the next generation (since in
``stille Post'' one has to whisper the word, thereby loosing information). Each agent
learns the language of the previous generation using some algorithm, and passes its variation
of the language onto the next generation. In an early experiment using this paradigm, Bartlett
had one participant draw an elk, and after some generations ended up with the drawing of a cat \parencite{bartlett1932remembering}.

\subsection*{Bayesian Rational Agents}

While the standard account is to model agents as artificial neural networks \parencite{kirby2002emergence}, lately Bayesian models have
come into fashion \parencite{perfors2011tutorial}. They offer some unique benefits to modellers.
The agent's innate biases towards a certain language are explicitly encoded in the P(h). The
information that is passed on to the agent from the previous generation is P(d). The agent
computes the likelihood of the utterance given a certain language, P(d|h), for each language. Via
Bayes' Rule

\begin{equation}
       P(h|d) = \frac{P(h) P(d|h)}{\sum_{h} P(h) P(d|h)}
\end{equation}

the agent computes the posterior distribution. Given the posterior distributions for each language, describing the probability of the data given each language, the agent picks one language using the specific learning algorithm. The agent then
outputs some utterances using this language, serving as input for the next generation.\footnote{while assumptions of Bayesian rationality are controversial \parencite{marcus2013robust}, experiments in this domain \parencite{kalish2007iterated} show that they might be reasonable.}
\\

Each learner sees some data, forms a hypothesis and then produces data according to that hypothesis, which serves as input for the next generation. This process across generations constitutes a markov chain, where the probability of a learner choosing a certain hypothesis, that is, acquire a certain language, depends only on the hypothesis chosen by the previous learner. We can construct a transition matrix which describes the probabilities that each hypothesis will lead to itself or any other hypothesis in the next generation \parencite{ferdinand2008language}. This matrix holds all the possible trajectories of the iterated learning model. Once the model has converged, we can take the first eigenvector of the transition matrix, which yields the stationary distribution. The stationary distribution can be thought of as the expected distribution of languages that emerged due to cultural evolution \parencite{kirby2007innateness}.

\subsection*{Previous Results}
\textcite{griffiths2005bayesian} demonstrated that iterated learning results in convergence to the prior distributions of hypothesis, i.e. the innate biases reflecting general cognitive capacities. Stated differently, the expected distribution of languages is proportional to the prior distribution, suggesting that language evolution is very much influenced by learning. It thus gives a detailed mechanistic account of how languages evolve through cultural transmission without relying on natural selection. However, by using not ``proportional sampling'' but MAP\footnote{MAP is a point estimate which has the maximum a posteriori probability.} as learning algorithm, \textcite{kirby2007innateness} found that the prior is not a good predictor of the resulting distribution of languages. Instead, the relation between prior and posterior gets obscured by iterated learning; that is, the number of training examples - the cultural bottleneck - determines the posterior distribution. We can thus conclude that the type of learning algorithm is crucial in this simulation studies. When learners use a form of Gibbs sampling, the prior converges to the posterior. When MAP is used, the posterior is affected both by the prior and by the amount of information transmitted \parencite{griffiths2007language}.\\

In the studies mentioned above, each generation only consists of one individual. However, when learners learn from multiple individuals - a situation much closer resembling real life - different results emerge. \textcite{smith2009iterated} showed that the straightforward mapping from prior to posterior vanished when considering multiple agents per generation, and urges modellers to use a more realistic setting, possibly incorporating social structures.

\subsection*{Our approach}
We want to first replicate the results found when using one individual per generation using both Gibbs sampling and MAP as learning algorithm. More specifically, we want to investigate compositionality as a universal language feature. We will write our simulation in Python using the popular \textbf{NumPy} library, and possibly \textbf{PyMC} for Gibbs Sampling.



\newpage
\printbibliography
\end{document}