\documentclass[12pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[american]{babel}
\usepackage{tikz}
\usepackage{csquotes}
\usepackage{amsmath}

\usepackage{color}
\usepackage{attrib}
\usepackage{blindtext}
\usepackage[hidelinks]{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black, % references (e.g. pages)
	linkcolor=black, %table of content links etc.
	urlcolor=blue
}

\usepackage[backend=biber, style=apa]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\let\oldequation=\equation
\let\endoldequation=\endequation
\renewenvironment{equation}{\vspace{5mm}\begin{oldequation}}{\end{oldequation}\vspace{5mm}}

\setlength\bibitemsep{0.5ex}
\setlength\bibnamesep{1.2ex}
\addbibresource{bibliography.bib}

\begin{document}
\section*{Project Sketch - Dablander \& GÃ¶rner}

\subsection*{Introduction}
``One key challenge for cognitive science'', \textcite[p. 5241]{kirby2007innateness} note, ``is to explain
the structure of human language.'' One way of sheding light on this question is to consider
how children acquire language. Traditional, bio-linguistic accounts (Pinker \& Bloom, 1990),
argue that most of our language knowledge is innate, formed by natural selection. In its extreme form, most of language learning is considered illusory (Smith, 2011). Recently, these viewpoints have been revised, and focus has been put on cultural factors. Since
languages are culturally transmitted, they are potentially under selection for learnability.
The basic argument is that languages adopt to become learnable by the agents, \textcolor{red}{I dont like this sentence...Implies somehow that language, in the first place was not learnable. I think one cannot say it adopts TO DO SOMETHING. It adopts because of the properties of the (dynamical) system it is} which leads to
universal features like regularization and compositionality. The knowledge of language of one
generation gets transmitted to the next generation and so on. The innate bias of the Agent in combination with cultural transmission and ontgenetical learning form the language that we end up with 
%\textcolor{red}{not entirely..I think thats the point. Its all about the interaction between the inateness, the cultural transmission and ontogenetical learning - all these elements contribute somehow to the language shaping}.

\subsection*{Iterated Learning Model}

This process was formalized by Kirby in 1999 and termed "Iterated Learning Model". The
three core features of this model \parencite{ferdinand2008language} are

\begin{enumerate}
  \item Learning Algorithm
  \item some form of information which is the input / output of the algorithm
  \item structured transmission of information
\end{enumerate}

Put crudely, this model formalizes games like ``stille Post''. One agent starts with
a word, say elk, and passes this word on to the next agent. Each agent represents
a generation, and the word passed on represents a certain language. Not all information of
the specific language (here the word "elk") gets passed onto the next generation (since in
``stille Post'' one has to whisper the word, thereby loosing information). Each agent
learns the language of the previous generation using some algorithm, and passes its variation
of the language onto the next generation. In an early experiment using this paradigm, Bartlett
had one participant draw an elk, and after some generations ended up with the drawing of a cat \parencite{bartlett1932remembering}.
Summing up, the ILM contains language-using agents, language-learning agents, meaning-space and utterance space \parencite{kirby2002emergence}. 

\subsection*{Bayesian Rational Agents}

While the standard account is to model agents as artificial neural networks \parencite{kirby2002emergence}, lately Bayesian models have
come into fashion \parencite{perfors2011tutorial}. They offer some unique benefits to modellers.
The agent's innate biases towards a certain language are explicitly encoded in the P(h). The
information that is passed on to the agent from the previous generation is P(d). The agent
computes the likelihood of the utterance given a certain language, P(d|h), for each language. Via
Bayes' Rule

\begin{equation}
       P(h|d) = \frac{P(h) P(d|h)}{\sum_{h} P(h) P(d|h)}
\end{equation}

the agent computes the posterior distribution. Given the posterior distributions for each language, describing the probability of the data given each language, the agent derives a language using the specific learning algorithm. The agent then
outputs some utterances using this language, serving as input for the next generation.\footnote{while assumptions of Bayesian rationality are controversial \parencite{marcus2013robust}, experiments in this domain \parencite{kalish2007iterated} show that they might be reasonable.}
\\

In other words, each learner sees some data, forms a hypothesis and then produces data according to that hypothesis, which serves as input for the next generation. This process across generations can be modeled as a markov chain, where the probability of a learner choosing a certain hypothesis, that is, acquire a certain language, depends only on the hypothesis chosen by the previous learner. We can construct a transition matrix which %\textcolor{red}{contains these probabilities and which then is applied to the language, creating } 
describes the probabilities that each hypothesis will lead to itself or any other hypothesis in the next generation \parencite{ferdinand2008language}. This matrix holds all the possible trajectories of the iterated learning model. 
%Once the model has converged, 
The first eigenvector of the transition matrix, is under certain conditions proportional to the stationary distribution. 
%The stationary distribution can be thought of as the expected distribution of languages that emerged due to cultural evolution \parencite{kirby2007innateness}. 
This specific feature of the ILM is problematic regarding natural languages since those do obviously not converge at all.

\subsection*{Previous Results}
\textcite{griffiths2005bayesian} demonstrated that iterated learning results in convergence to the prior distributions of hypothesis, i.e. the innate biases reflecting general cognitive capacities. Stated differently, the expected distribution of languages is proportional to the prior distribution.
%suggesting that language evolution is very much influenced by learning under innate constraints. 
It thus gives a detailed mechanistic account of how languages evolve through cultural transmission in interaction with individual learning and evolutionary constraints.
% without relying on natural selection. 
However, by using not ``proportional sampling'' but MAP\footnote{MAP is a point estimate which has the maximum a posteriori probability.} as learning algorithm, \textcite{kirby2007innateness} found that the prior is not a good predictor of the resulting distribution of languages. Instead, the relation between prior and posterior gets obscured by iterated learning; that is, the number of training examples - the cultural bottleneck - determines the posterior distribution. We can thus conclude that the type of learning algorithm is crucial in this simulation studies. When learners use a form of Gibbs sampling, the prior converges to the posterior. When MAP is used, the posterior is affected both by the prior and by the amount of information transmitted \parencite{griffiths2007language}.\\

In the studies mentioned above, each generation only consists of one individual. However, when learners learn from multiple individuals - a situation much closer resembling real life - different results emerge. \textcite{smith2009iterated} showed that the straightforward mapping from prior to posterior vanished when considering multiple agents per generation, and urges modellers to use a more realistic setting, possibly incorporating social structures.

\subsection*{Our approach}
We want to first replicate the results found when using one individual per generation using both Gibbs sampling and MAP as learning algorithm. More specifically, we want to investigate compositionality as a universal language feature. We will write our simulation in Python using the popular \textbf{NumPy} library, and possibly \textbf{PyMC} for Gibbs Sampling.



\newpage
\printbibliography
\end{document}